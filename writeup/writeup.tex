\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{enumitem}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{fontspec}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{float}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{textgreek}

\captionsetup{labelformat=empty}

% Configure listings (no box, no line numbers)
\lstset{
  language=Python,
  basicstyle=\ttfamily\small,
  keywordstyle=\color{blue},
  commentstyle=\color{gray},
  stringstyle=\color{red},
  showstringspaces=false,
  frame=none,                % no box
  breaklines=false,          % don't allow line breaks
  keepspaces=true,
  aboveskip=1em,
  belowskip=1em
}

% Optional: to prevent page breaking manually
\newenvironment{tightcode}
  {\par\noindent\vspace{1ex}\begin{minipage}{\linewidth}}
  {\end{minipage}\vspace{1ex}}

\title{CS336 Assignment 2}
\author{Dmitry Frumkin}
\date{\today}

\begin{document}

\maketitle
\tableofcontents

\setcounter{section}{1}
\setcounter{subsection}{0}

\subsection{Profiling and Benchmarking}

\textbf{Code:} \texttt{cs336\_systems/benchmark.py}

\subsubsection*{Problem (benchmarking\_script)}

\textbf{Code:} 
\begin{itemize}
\item \texttt{cs336\_systems/benchmark.sh}
\item \texttt{cs336\_systems/benchmark.ipynb}
\end{itemize}

\begin{enumerate}[label=(\alph*)]
\item \textbf{Write a script to perform basic end-to-end benchmarking of the forward and backward passes in
your model.}

I used an H-100 via \href{https://lambda.ai/}{lambda.ai} and performed 
sweeps over (forward or forward-backward, model size, context window size, number of warmup iterations).
Every configuration was a separate process.  For cleaner results, I reset the GPU in the beginning of every run.

\item \textbf{Time the forward and backward passes for the model sizes described in §1.1.2. Use 5 warmup
steps and compute the average and standard deviation of timings over 10 measurement steps.
How long does a forward pass take? How about a backward pass? Do you see high variability
across measurements, or is the standard deviation small?}

The forward pass takes longer in the training mode (also includes the loss), but usually not by much.  
The backward pass takes longer than the forward pass.
Computation expectedly takes longer for larger models and larger contexts (scaling linearly with the context length).
The standard deviation is generally small.

\item \textbf{One caveat of benchmarking is not performing the warm-up steps. Repeat your analysis without
the warm-up steps. How does this affect your results? Why do you think this happens? Also try
to run the script with 1 or 2 warm-up steps. Why might the result still be different?}

Without warm-up, the running time as well as the standard deviation are much higher.  In general, increasing the number
of warmup steps led to a smaller standard deviation, but not always.  Having more warmup steps helps because
there is some intentional lazy initialization and auto-tuning (different behavior for frequent vs. one-off tasks).

\end{enumerate}

\input{tables/benchmark.tex}

\subsubsection*{Problem (nsys\_profile)}

\textbf{Code:} 
\begin{itemize}
\item \texttt{cs336\_systems/profile.sh}
\item \texttt{cs336\_systems/profile\_reports.sh}
\item \texttt{cs336\_systems/profile.ipynb}
\end{itemize}

\begin{enumerate}[label=(\alph*)]
    \item \textbf{What is the total time spent on your forward pass? Does it match what we had measured before
with the Python standard library?}

\input{tables/profile_forw.tex}

The forward time matches what we measured earlier (is a little higher).

    \item \textbf{What CUDA kernel takes the most cumulative GPU time during the forward pass? How many
times is this kernel invoked during a single forward pass of your model? Is it the same kernel
that takes the most runtime when you do both forward and backward passes?}

For an xl model with context length 512, \texttt{sm80\_xmma\_gemm\_f32f32...} matrix multiplication kernels
took the most time (123 calls for the most popular one).  In the backward pass, the most time is taken by a similar matrix multiplication kernel
(a transposed version on the forward pass and a non-transposed one on the backward pass).

    \item \textbf{What other kernels besides matrix multiplies do you see accounting for non-trivial CUDA runtime in the forward
pass?}

I see elementwise operations, such as multiplication and addition, (\texttt{at::native::elementwise\_kernel and at::native::vectorized\_elementwise\_kernel}) 
and reductions, such as mean and max (\texttt{at::native::reduce\_kernel}).

    \item \textbf{Profile running one complete training step with your implementation of AdamW (i.e., the forward
pass, computing the loss and running a backward pass, and finally an optimizer step, as you’d do
during training). How does the fraction of time spent on matrix multiplication change, compared
to doing inference (forward pass only)? How about other kernels?}

For an xl model with context length 512, during inference, 4 matrix multiplication kernels take about 80\% of the time with the rest going to elementwise (mostly)
and reduction operations.  Over a training step, there are many more kernels run (including matrix multiplication ones): matrix multiplication still dominates, but newenvironment
takes only about 60\% of the time, while pointwise operations take a relatively larger share of time.

    \item \textbf{Compare the runtime of the softmax operation versus the matrix multiplication operations within
the self-attention layer of your model during a forward pass. How does the difference in runtimes
compare to the difference in FLOPs?}

For an xl model with context length 512, focusing on \texttt{scaled\_dot\_product\_attention}, softmax takes nearly as much time as computing attention
scores and the final matrix multiplication combined (556 ns vs 388 and 236 ns).  This is very high given that the fraction of FLOPs taken by softmax is tiny 
($\sim 512^2$ vs $\sim 1280 \times 512^2$ for the other two operations).  The reason for high running time is that unlike highly optimized
matrix multiplications, softmax is very memory-bound.

\end{enumerate}

\subsubsection*{Problem (mixed\_precision\_accumulation)}
\begin{tightcode}
\begin{lstlisting}
import torch

s = torch.tensor(0,dtype=torch.float32)
for i in range(1000):
    s += torch.tensor(0.01,dtype=torch.float32)
print(s)

s = torch.tensor(0,dtype=torch.float16)
for i in range(1000):
    s += torch.tensor(0.01,dtype=torch.float16)
print(s)

s = torch.tensor(0,dtype=torch.float32)
for i in range(1000):
    s += torch.tensor(0.01,dtype=torch.float16)
print(s)

s = torch.tensor(0,dtype=torch.float32)
for i in range(1000):
    x = torch.tensor(0.01,dtype=torch.float16)
    s += x.type(torch.float32)
print(s)

tensor(10.0001)
tensor(9.9531, dtype=torch.float16)
tensor(10.0021)
tensor(10.0021)
\end{lstlisting}
\end{tightcode}

There are two sources of error: from 0.01 stored in binary (higher in float16) and from the result of addition
stored in binary (again higher in float16).  In the first case (everything in float32), we get minimal
error; in the second case (everything in float16) we get the highest error from both sources; in the third and
fourth cases (equivalent: implicit or explicity upcasting to float32) we get only the higher error from storing
0.01 in float16, but lower error from addition.

\subsubsection*{Problem (benchmarking\_mixed\_precision)}

\textbf{Code:} 

\begin{itemize}
\item \texttt{cs336\_systems/benchmark\_mp.sh}
\item \texttt{cs336\_systems/mixed\_precision.ipynb}
\end{itemize}

\begin{enumerate}[label=(\alph*)]

    \item Parameters and gradients are always in float32 for reasons demonstrated in the previous problem.  
    Activations are computed in float16 for linear layers (fc1, fc2) and relu, but float32 for LayerNorm.
    Logits are activations of fc2 (float16) and the loss is float32 because it's usually numerically sensitive (MSE, Cross-Entropy), but could in theory be float16 (e.g. if we take the maximum).

    \item LayerNorm contains sensitive operations: accumulations (mean, standard deviation), epsilon addition and square root.
    While bfloat16 would help avoid over- and underflows, it has fewer bits for the mantissa leading to loss in precision.  
    Therefore, we always use float32 for LayerNorm. In theory, hardware could fuse LayerNorm computations to 
    produce float16/bfloat16 activations, but the benefits would be much smaller than from the fused matmul.

    \item Mixed precision did not help with OOM issues; however,
    as the model and the context window sizes increased, we got super-linear savings in running time as a result of better register / cache / memory
    utilization and more efficient tensor cores / kernels.  Surprisingly, with mixed precision, 
    the 2.7B model was often faster than the xl or even large model at small context window sizes.

    \input{tables/mixed_precision.tex}

\end{enumerate}

\subsubsection*{Problem (memory\_profiling)}

\textbf{Code:} \texttt{cs336\_systems/profile\_memory.sh}

\begin{enumerate}[label=(\alph*)]
    
    \item \textbf{How do your memory timelines look like? Can you tell which stage is running based on the peaks you see?}
    
For the forward pass during inference, most memory has already been allocated and we see only small bursts.  The largest peaks corresponds to \texttt{softmax}.  
Smaller ones correspond to other transformer block operations, such as \texttt{SiLU}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/forward.png}
    \caption{Forward pass}
\end{figure}

For the full step, we see a small spike for \texttt{zero\_grad} in the beginning, then a gradual increase in memory use during the forward pass 
(store activations with spikes for softmax, etc.).
During the backward pass, we deallocate activations while allocating / reusing memory for gradients.  
At context length 128, the peak is during the backward pass (fewer attention activations), whereas at 512, the peak is achieved during the forward pass.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/training_step.png}
    \caption{Training step}
\end{figure}

    \item \textbf{What is the peak memory usage of each context length when doing a forward pass? What about when doing a full training step?}

    There is not much difference between the context lengths 128 and 256 because in both cases memory is mostly used by the backward pass.

    \input{tables/peak_memory.tex}
    
    \item \textbf{Does mixed-precision significantly affect memory usage?}

Not significantly.  We instantiate the model in float32 and then use autocast for the forward pass, so we increase memory consumption during warmup - before taking measurements. 
At the same time, it decreases the size of activations, which is important only with larger contexts (we see the difference at 512).

    \item \textbf{At our reference hyperparameters, what is the size of a tensor of activations in the Transformer residual stream, in single-precision?}

    $\frac{d_k \times context\_length \times batch\_size \times bytes\_in\_float}{bytes\_in\_megabyte}=\frac{2560 \times 512 \times 4 \times 4}{1024^2}=20$ (MB).

    \item \textbf{What is the size of the largest allocations shown? Looking through the stack trace, can you tell where those allocations come from?}
    
    The largest alloctions have size 128 MB and come from attention's softmax.

    $\frac{batch\_size \times num\_heads \times context\_size^2 \times bytes\_in\_float}{bytes\_in\_megabyte} = \frac{4 \times 32 \times 512^2 \times 4}{1024^2} = 128$

\end{enumerate}

\subsection{Optimizing Attention with FlashAttention-2}
\subsubsection*{Problem (pytorch\_attention)}

\subsection{Benchmarking JIT-Compiled Attention}
\subsubsection*{Problem (torch\_compile)}
\subsubsection*{Problem (flash\_forward)}
\subsubsection*{Problem (flash\_backward)}
\subsubsection*{Problem (flash\_benchmarking)}

\setcounter{section}{2}
\setcounter{subsection}{0}

\subsection{Single-Node Distributed Communication in PyTorch}
\subsubsection*{Problem (distributed\_communication\_single\_node)}

\subsection{A Naïve Implementation of Distributed Data Parallel Training}
\subsubsection*{Problem (naive\_ddp)}
\subsubsection*{Problem (naive\_ddp\_benchmarking)}

\subsection{Improving Upon the Minimal DDP Implementation}
\subsubsection*{Problem (minimal\_ddp\_flat\_benchmarking)}
\subsubsection*{Problem (ddp\_overlap\_individual\_parameters)}
\subsubsection*{Problem (ddp\_overlap\_individual\_parameters\_benchmarking)}
\subsubsection*{Problem (ddp\_overlap\_bucketed)}
\subsubsection*{Problem (ddp\_bucketed\_benchmarking)}

\subsection{4D Parallelism}
\subsubsection*{Problem (communication\_accounting)}

{
\renewcommand{\thesubsection}{3}
\subsection{Optimizer State Sharding}
}

\subsubsection*{Problem (optimizer\_state\_sharding)}
\subsubsection*{Problem (optimizer\_state\_sharding\_accounting)}

\end{document}