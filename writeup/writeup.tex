\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{enumitem}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{fontspec}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{float}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{textgreek}

\captionsetup{labelformat=empty}

% Configure listings (no box, no line numbers)
\lstset{
  language=Python,
  basicstyle=\ttfamily\small,
  keywordstyle=\color{blue},
  commentstyle=\color{gray},
  stringstyle=\color{red},
  showstringspaces=false,
  frame=none,                % no box
  breaklines=false,          % don't allow line breaks
  keepspaces=true,
  aboveskip=1em,
  belowskip=1em
}

% Optional: to prevent page breaking manually
\newenvironment{tightcode}
  {\par\noindent\vspace{1ex}\begin{minipage}{\linewidth}}
  {\end{minipage}\vspace{1ex}}

\title{CS336 Assignment 2}
\author{Dmitry Frumkin}
\date{\today}

\begin{document}

\maketitle
\tableofcontents

\setcounter{section}{1}
\setcounter{subsection}{0}

\subsection{Profiling and Benchmarking}
\subsubsection*{Problem (benchmarking\_script)}

\begin{enumerate}[label=(\alph*)]
\item \textbf{Write a script to perform basic end-to-end benchmarking of the forward and backward passes in
your model.}

\textbf{Code:} \texttt{cs336\_systems/benchmark.py}, \texttt{cs336\_systems/benchmark.sh}, \texttt{cs336\_systems/benchmark.ipynb}

In this and subsequent problems I used an H-100 via \href{https://lambda.ai/}{lambda.ai} and hydra to run
hyperparameter sweeps.  Here, the sweeps were over (model size, context window size, number of warmup iterations).
Two sweeps were run:

\begin{itemize}
\item training: forward pass (including loss computation) and backward pass;
\item inference: forward pass in inference mode.
\end{itemize}

Every benchmark run within each sweep is a separate process.  For cleaner results, I reset the GPU in the beginning of every run.
The set-up is probably different from what Stanford students had.

The difference between inference and training forward passes is minimal, except for the much larger memory 
footprint during training, which resulted in the larger models running out of memory in training.

\item \textbf{Time the forward and backward passes for the model sizes described in §1.1.2. Use 5 warmup
steps and compute the average and standard deviation of timings over 10 measurement steps.
How long does a forward pass take? How about a backward pass? Do you see high variability
across measurements, or is the standard deviation small?}

The backward takes longer than the forward pass, and the difference is more pronounced for larger models ($2 \times$).
Computation expectedly takes longer for larger models and larger contexts (scaling linearly with the context length).
The standard deviation is generally small, though it was for some reason unusually high for the large model with the context length of 128.

\item \textbf{One caveat of benchmarking is not performing the warm-up steps. Repeat your analysis without
the warm-up steps. How does this affect your results? Why do you think this happens? Also try
to run the script with 1 or 2 warm-up steps. Why might the result still be different?}

Without warm-up, the running time as well as the standard deviation are much higher.  Particularly affected
are the first runs of training and inference sweeps (small model, context 128, warm-up 0): most likely there
is some extra initialization for the new task even though we reset the GPU every time.  In general, increasing the number
of warmup steps led to a smaller standard deviation, but not always.  Having more warmup steps helps because
there is some intentional lazy initialization and auto-tuning (different behavior for frequent vs. one-off tasks).

\end{enumerate}

\input{tables/benchmark.tex}

\subsubsection*{Problem (nsys\_profile)}
\subsubsection*{Problem (mixed\_precision\_accumulation)}
\begin{tightcode}
\begin{lstlisting}
import torch

s = torch.tensor(0,dtype=torch.float32)
for i in range(1000):
    s += torch.tensor(0.01,dtype=torch.float32)
print(s)

s = torch.tensor(0,dtype=torch.float16)
for i in range(1000):
    s += torch.tensor(0.01,dtype=torch.float16)
print(s)

s = torch.tensor(0,dtype=torch.float32)
for i in range(1000):
    s += torch.tensor(0.01,dtype=torch.float16)
print(s)

s = torch.tensor(0,dtype=torch.float32)
for i in range(1000):
    x = torch.tensor(0.01,dtype=torch.float16)
    s += x.type(torch.float32)
print(s)

tensor(10.0001)
tensor(9.9531, dtype=torch.float16)
tensor(10.0021)
tensor(10.0021)
\end{lstlisting}
\end{tightcode}

There are two sources of error: from 0.01 stored in binary (higher in float16) and from the result of addition
stored in binary (again higher in float16).  In the first case (everything in float32), we get minimal
error; in the second case (everything in float16) we get the highest error from both sources; in the third and
fourth cases (equivalent: implicit or explicity upcasting to float32) we get only the higher error from storing
0.01 in float16, but lower error from addition.

\subsubsection*{Problem (benchmarking\_mixed\_precision)}
\subsubsection*{Problem (memory\_profiling)}

\subsection{Optimizing Attention with FlashAttention-2}
\subsubsection*{Problem (pytorch\_attention)}

\subsection{Benchmarking JIT-Compiled Attention}
\subsubsection*{Problem (torch\_compile)}
\subsubsection*{Problem (flash\_forward)}
\subsubsection*{Problem (flash\_backward)}
\subsubsection*{Problem (flash\_benchmarking)}

\setcounter{section}{2}
\setcounter{subsection}{0}

\subsection{Single-Node Distributed Communication in PyTorch}
\subsubsection*{Problem (distributed\_communication\_single\_node)}

\subsection{A Naïve Implementation of Distributed Data Parallel Training}
\subsubsection*{Problem (naive\_ddp)}
\subsubsection*{Problem (naive\_ddp\_benchmarking)}

\subsection{Improving Upon the Minimal DDP Implementation}
\subsubsection*{Problem (minimal\_ddp\_flat\_benchmarking)}
\subsubsection*{Problem (ddp\_overlap\_individual\_parameters)}
\subsubsection*{Problem (ddp\_overlap\_individual\_parameters\_benchmarking)}
\subsubsection*{Problem (ddp\_overlap\_bucketed)}
\subsubsection*{Problem (ddp\_bucketed\_benchmarking)}

\subsection{4D Parallelism}
\subsubsection*{Problem (communication\_accounting)}

{
\renewcommand{\thesubsection}{3}
\subsection{Optimizer State Sharding}
}

\subsubsection*{Problem (optimizer\_state\_sharding)}
\subsubsection*{Problem (optimizer\_state\_sharding\_accounting)}

\end{document}