\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{enumitem}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{fontspec}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{float}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{textgreek}

\captionsetup{labelformat=empty}

% Configure listings (no box, no line numbers)
\lstset{
  language=Python,
  basicstyle=\ttfamily\small,
  keywordstyle=\color{blue},
  commentstyle=\color{gray},
  stringstyle=\color{red},
  showstringspaces=false,
  frame=none,                % no box
  breaklines=false,          % don't allow line breaks
  keepspaces=true,
  aboveskip=1em,
  belowskip=1em
}

% Optional: to prevent page breaking manually
\newenvironment{tightcode}
  {\par\noindent\vspace{1ex}\begin{minipage}{\linewidth}}
  {\end{minipage}\vspace{1ex}}

\title{CS336 Assignment 2}
\author{Dmitry Frumkin}
\date{\today}

\begin{document}

\maketitle
\tableofcontents

\setcounter{section}{1}
\setcounter{subsection}{0}

\subsection{Profiling and Benchmarking}
\subsubsection*{Problem (benchmarking\_script)}

\begin{enumerate}[label=(\alph*)]
\item \textbf{Write a script to perform basic end-to-end benchmarking of the forward and backward passes in
your model.}

\textbf{Code:} \texttt{cs336\_systems/benchmark.py}, \texttt{cs336\_systems/benchmark.sh}, \texttt{cs336\_systems/benchmark.ipynb}

In this and subsequent problems I used an A-100 via \href{https://lambda.ai/}{lambda.ai} and performed 
sweeps over (forward only, model size, context window size, number of warmup iterations).
Every configuration was a separate process.  For cleaner results, I reset the GPU in the beginning of every run.

The difference between inference and training forward passes is minimal, except for the much larger memory 
footprint during training, which resulted in out-of-memory errors for larger models and contexts.

\item \textbf{Time the forward and backward passes for the model sizes described in §1.1.2. Use 5 warmup
steps and compute the average and standard deviation of timings over 10 measurement steps.
How long does a forward pass take? How about a backward pass? Do you see high variability
across measurements, or is the standard deviation small?}

The backward takes longer than the forward pass.
Computation expectedly takes longer for larger models and larger contexts (scaling linearly with the context length).
The standard deviation is generally small.

\item \textbf{One caveat of benchmarking is not performing the warm-up steps. Repeat your analysis without
the warm-up steps. How does this affect your results? Why do you think this happens? Also try
to run the script with 1 or 2 warm-up steps. Why might the result still be different?}

Without warm-up, the running time as well as the standard deviation are much higher.  In general, increasing the number
of warmup steps led to a smaller standard deviation, but not always.  Having more warmup steps helps because
there is some intentional lazy initialization and auto-tuning (different behavior for frequent vs. one-off tasks).

\end{enumerate}

\input{tables/benchmark.tex}

\subsubsection*{Problem (nsys\_profile)}

\begin{enumerate}[label=(\alph*)]
    \item \textbf{What is the total time spent on your forward pass? Does it match what we had measured before
with the Python standard library?}

\input{tables/profile_forw.tex}

The forward time matches what we measured earlier (is a little higher).

    \item \textbf{What CUDA kernel takes the most cumulative GPU time during the forward pass? How many
times is this kernel invoked during a single forward pass of your model? Is it the same kernel
that takes the most runtime when you do both forward and backward passes?}

For an xl model with context length 512, \texttt{ampere\_sgemm\_128x64\_tn} (matrix multiplication kernel operating on 128 rows × 64 columns in single precision) 
takes the most time during the forward pass and gets called 52 times.  In the backward pass, the most time is taken by a similar matrix multiplication
\texttt{ampere\_sgemm\_64x32\_sliced1x4\_nn}, but for both steps combined \texttt{ampere\_sgemm\_128x64\_tn} still takes the most time.

    \item \textbf{What other kernels besides matrix multiplies do you see accounting for non-trivial CUDA runtime in the forward
pass?}

I see elementwise operations, such as multiplication and addition, (\texttt{at::native::elementwise\_kernel and at::native::vectorized\_elementwise\_kernel}) 
and reductions, such as mean and max (\texttt{at::native::reduce\_kernel}).

    \item \textbf{Profile running one complete training step with your implementation of AdamW (i.e., the forward
pass, computing the loss and running a backward pass, and finally an optimizer step, as you’d do
during training). How does the fraction of time spent on matrix multiplication change, compared
to doing inference (forward pass only)? How about other kernels?}

For an xl model with context length 512, during inference, 4 matrix multiplication kernels take 84.5\% of the time with the rest going to elementwise (mostly)
and reduction operations.  Over a training step, there are many more kernels run (including matrix multiplication ones): matrix multiplication still dominates
with 74.2\%, while pointwise operations take a relatively larger share of time.

    \item \textbf{Compare the runtime of the softmax operation versus the matrix multiplication operations within
the self-attention layer of your model during a forward pass. How does the difference in runtimes
compare to the difference in FLOPs?}

For an xl model with context length 512, focusing on \texttt{scaled\_dot\_product\_attention}, softmax takes 714 ns vs 445 ns for computing attention
scores and 491 ns for the final matrix multiplication.  This is very high given that the fraction of FLOPs taken by softmax is tiny 
($\sim 512^2$ vs $\sim 1280 \times 512^2$ for the other two operations).  The reason for high running time is that unlike highly optimized
matrix multiplications, softmax is very memory-bound.

\end{enumerate}

\subsubsection*{Problem (mixed\_precision\_accumulation)}
\begin{tightcode}
\begin{lstlisting}
import torch

s = torch.tensor(0,dtype=torch.float32)
for i in range(1000):
    s += torch.tensor(0.01,dtype=torch.float32)
print(s)

s = torch.tensor(0,dtype=torch.float16)
for i in range(1000):
    s += torch.tensor(0.01,dtype=torch.float16)
print(s)

s = torch.tensor(0,dtype=torch.float32)
for i in range(1000):
    s += torch.tensor(0.01,dtype=torch.float16)
print(s)

s = torch.tensor(0,dtype=torch.float32)
for i in range(1000):
    x = torch.tensor(0.01,dtype=torch.float16)
    s += x.type(torch.float32)
print(s)

tensor(10.0001)
tensor(9.9531, dtype=torch.float16)
tensor(10.0021)
tensor(10.0021)
\end{lstlisting}
\end{tightcode}

There are two sources of error: from 0.01 stored in binary (higher in float16) and from the result of addition
stored in binary (again higher in float16).  In the first case (everything in float32), we get minimal
error; in the second case (everything in float16) we get the highest error from both sources; in the third and
fourth cases (equivalent: implicit or explicity upcasting to float32) we get only the higher error from storing
0.01 in float16, but lower error from addition.

\subsubsection*{Problem (benchmarking\_mixed\_precision)}


\begin{enumerate}[label=(\alph*)]

    \item Parameters and gradients are always in float32 for reasons demonstrated in the previous problem.  
    Activations are computed in float16 for linear layers (fc1, fc2) and relu, but float32 for LayerNorm.
    Logits are activations of fc2 (float16) and the loss is float32 because it's usually numerically sensitive (MSE, Cross-Entropy), but could in theory be float16 (e.g. if we take the maximum).

    \item LayerNorm contains sensitive operations: accumulations (mean, standard deviation), epsilon addition and square root.
    While bfloat16 would help avoid over- and underflows, it has fewer bits for the mantissa leading to loss in precision.  
    Therefore, we always use float32 for LayerNorm. In theory, hardware could fuse LayerNorm computations to 
    produce float16/bfloat16 activations, but the benefits would be much smaller than from the fused matmul.

    \item Mixed precision did not help with OOM issues on A-100 with 40GB GPU memory (the savings are limited by the factor of 2); however,
    as the model and the context window sizes increased, we got super-linear savings in running time.  Surprisingly, with mixed precision, 
    the 2.7B model's inference was faster than that of the xl and even large model at small context window sizes.

    \input{tables/mixed_precision.tex}

\end{enumerate}

\subsubsection*{Problem (memory\_profiling)}

\subsection{Optimizing Attention with FlashAttention-2}
\subsubsection*{Problem (pytorch\_attention)}

\subsection{Benchmarking JIT-Compiled Attention}
\subsubsection*{Problem (torch\_compile)}
\subsubsection*{Problem (flash\_forward)}
\subsubsection*{Problem (flash\_backward)}
\subsubsection*{Problem (flash\_benchmarking)}

\setcounter{section}{2}
\setcounter{subsection}{0}

\subsection{Single-Node Distributed Communication in PyTorch}
\subsubsection*{Problem (distributed\_communication\_single\_node)}

\subsection{A Naïve Implementation of Distributed Data Parallel Training}
\subsubsection*{Problem (naive\_ddp)}
\subsubsection*{Problem (naive\_ddp\_benchmarking)}

\subsection{Improving Upon the Minimal DDP Implementation}
\subsubsection*{Problem (minimal\_ddp\_flat\_benchmarking)}
\subsubsection*{Problem (ddp\_overlap\_individual\_parameters)}
\subsubsection*{Problem (ddp\_overlap\_individual\_parameters\_benchmarking)}
\subsubsection*{Problem (ddp\_overlap\_bucketed)}
\subsubsection*{Problem (ddp\_bucketed\_benchmarking)}

\subsection{4D Parallelism}
\subsubsection*{Problem (communication\_accounting)}

{
\renewcommand{\thesubsection}{3}
\subsection{Optimizer State Sharding}
}

\subsubsection*{Problem (optimizer\_state\_sharding)}
\subsubsection*{Problem (optimizer\_state\_sharding\_accounting)}

\end{document}