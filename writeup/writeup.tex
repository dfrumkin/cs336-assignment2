\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{enumitem}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{fontspec}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{float}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{textgreek}

\captionsetup{labelformat=empty}

% Configure listings (no box, no line numbers)
\lstset{
  language=Python,
  basicstyle=\ttfamily\small,
  keywordstyle=\color{blue},
  commentstyle=\color{gray},
  stringstyle=\color{red},
  showstringspaces=false,
  frame=none,                % no box
  breaklines=false,          % don't allow line breaks
  keepspaces=true,
  aboveskip=1em,
  belowskip=1em
}

% Optional: to prevent page breaking manually
\newenvironment{tightcode}
  {\par\noindent\vspace{1ex}\begin{minipage}{\linewidth}}
  {\end{minipage}\vspace{1ex}}

\title{CS336 Assignment 2}
\author{Dmitry Frumkin}
\date{\today}

\begin{document}

\maketitle
\tableofcontents

\setcounter{section}{1}
\setcounter{subsection}{0}

\subsection{Profiling and Benchmarking}

\textbf{Code:} \texttt{cs336\_systems/benchmark.py}

\subsubsection*{Problem (benchmarking\_script)}

\textbf{Code:} 
\begin{itemize}
\item \texttt{cs336\_systems/benchmark.sh}
\item \texttt{cs336\_systems/benchmark.ipynb}
\end{itemize}

\begin{enumerate}[label=(\alph*)]
\item \textbf{Write a script to perform basic end-to-end benchmarking of the forward and backward passes in
your model.}

I used an H-100 via \href{https://lambda.ai/}{lambda.ai} and performed 
sweeps over (forward or forward-backward, model size, context window size, number of warmup iterations).
Every configuration was a separate process.  For cleaner results, I reset the GPU in the beginning of every run.

\item \textbf{Time the forward and backward passes for the model sizes described in §1.1.2. Use 5 warmup
steps and compute the average and standard deviation of timings over 10 measurement steps.
How long does a forward pass take? How about a backward pass? Do you see high variability
across measurements, or is the standard deviation small?}

The forward pass takes longer in the training mode (also includes the loss), but usually not by much.  
The backward pass takes longer than the forward pass.
Computation expectedly takes longer for larger models and larger contexts (scaling linearly with the context length).
The standard deviation is generally small.

\item \textbf{One caveat of benchmarking is not performing the warm-up steps. Repeat your analysis without
the warm-up steps. How does this affect your results? Why do you think this happens? Also try
to run the script with 1 or 2 warm-up steps. Why might the result still be different?}

Without warm-up, the running time as well as the standard deviation are much higher.  In general, increasing the number
of warmup steps led to a smaller standard deviation, but not always.  Having more warmup steps helps because
there is some intentional lazy initialization and auto-tuning (different behavior for frequent vs. one-off tasks).

\end{enumerate}

\input{tables/benchmark.tex}

\subsubsection*{Problem (nsys\_profile)}

\textbf{Code:} 
\begin{itemize}
\item \texttt{cs336\_systems/profile.sh}
\item \texttt{cs336\_systems/profile\_reports.sh}
\item \texttt{cs336\_systems/profile.ipynb}
\end{itemize}

\begin{enumerate}[label=(\alph*)]
    \item \textbf{What is the total time spent on your forward pass? Does it match what we had measured before
with the Python standard library?}

\input{tables/profile_forw.tex}

The forward time matches what we measured earlier (is a little higher).

    \item \textbf{What CUDA kernel takes the most cumulative GPU time during the forward pass? How many
times is this kernel invoked during a single forward pass of your model? Is it the same kernel
that takes the most runtime when you do both forward and backward passes?}

For an xl model with context length 512, \texttt{sm80\_xmma\_gemm\_f32f32...} matrix multiplication kernels
took the most time (123 calls for the most popular one).  In the backward pass, the most time is taken by a similar matrix multiplication kernel
(a transposed version on the forward pass and a non-transposed one on the backward pass).

    \item \textbf{What other kernels besides matrix multiplies do you see accounting for non-trivial CUDA runtime in the forward
pass?}

I see elementwise operations, such as multiplication and addition, (\texttt{at::native::elementwise\_kernel and at::native::vectorized\_elementwise\_kernel}) 
and reductions, such as mean and max (\texttt{at::native::reduce\_kernel}).

    \item \textbf{Profile running one complete training step with your implementation of AdamW (i.e., the forward
pass, computing the loss and running a backward pass, and finally an optimizer step, as you’d do
during training). How does the fraction of time spent on matrix multiplication change, compared
to doing inference (forward pass only)? How about other kernels?}

For an xl model with context length 512, during inference, 4 matrix multiplication kernels take about 80\% of the time with the rest going to elementwise (mostly)
and reduction operations.  Over a training step, there are many more kernels run (including matrix multiplication ones): matrix multiplication still dominates, but newenvironment
takes only about 60\% of the time, while pointwise operations take a relatively larger share of time.

    \item \textbf{Compare the runtime of the softmax operation versus the matrix multiplication operations within
the self-attention layer of your model during a forward pass. How does the difference in runtimes
compare to the difference in FLOPs?}

For an xl model with context length 512, focusing on \texttt{scaled\_dot\_product\_attention}, softmax takes nearly as much time as computing attention
scores and the final matrix multiplication combined (556 ns vs 388 and 236 ns).  This is very high given that the fraction of FLOPs taken by softmax is tiny 
($\sim 512^2$ vs $\sim 1280 \times 512^2$ for the other two operations).  The reason for high running time is that unlike highly optimized
matrix multiplications, softmax is very memory-bound.

\end{enumerate}

\subsubsection*{Problem (mixed\_precision\_accumulation)}
\begin{tightcode}
\begin{lstlisting}
import torch

s = torch.tensor(0,dtype=torch.float32)
for i in range(1000):
    s += torch.tensor(0.01,dtype=torch.float32)
print(s)

s = torch.tensor(0,dtype=torch.float16)
for i in range(1000):
    s += torch.tensor(0.01,dtype=torch.float16)
print(s)

s = torch.tensor(0,dtype=torch.float32)
for i in range(1000):
    s += torch.tensor(0.01,dtype=torch.float16)
print(s)

s = torch.tensor(0,dtype=torch.float32)
for i in range(1000):
    x = torch.tensor(0.01,dtype=torch.float16)
    s += x.type(torch.float32)
print(s)

tensor(10.0001)
tensor(9.9531, dtype=torch.float16)
tensor(10.0021)
tensor(10.0021)
\end{lstlisting}
\end{tightcode}

There are two sources of error: from 0.01 stored in binary (higher in float16) and from the result of addition
stored in binary (again higher in float16).  In the first case (everything in float32), we get minimal
error; in the second case (everything in float16) we get the highest error from both sources; in the third and
fourth cases (equivalent: implicit or explicity upcasting to float32) we get only the higher error from storing
0.01 in float16, but lower error from addition.

\subsubsection*{Problem (benchmarking\_mixed\_precision)}

\textbf{Code:} 

\begin{itemize}
\item \texttt{cs336\_systems/benchmark\_mp.sh}
\item \texttt{cs336\_systems/mixed\_precision.ipynb}
\end{itemize}

\begin{enumerate}[label=(\alph*)]

    \item Parameters and gradients are always in float32 for reasons demonstrated in the previous problem.  
    Activations are computed in float16 for linear layers (fc1, fc2) and relu, but float32 for LayerNorm.
    Logits are activations of fc2 (float16) and the loss is float32 because it's usually numerically sensitive (MSE, Cross-Entropy), but could in theory be float16 (e.g. if we take the maximum).

    \item LayerNorm contains sensitive operations: accumulations (mean, standard deviation), epsilon addition and square root.
    While bfloat16 would help avoid over- and underflows, it has fewer bits for the mantissa leading to loss in precision.  
    Therefore, we always use float32 for LayerNorm. In theory, hardware could fuse LayerNorm computations to 
    produce float16/bfloat16 activations, but the benefits would be much smaller than from the fused matmul.

    \item Mixed precision did not help with OOM issues; however,
    as the model and the context window sizes increased, we got super-linear savings in running time as a result of better register / cache / memory
    utilization and more efficient tensor cores / kernels.  Surprisingly, with mixed precision, 
    the 2.7B model was often faster than the xl or even large model at small context window sizes.

    \input{tables/mixed_precision.tex}

\end{enumerate}

\subsubsection*{Problem (memory\_profiling)}

\textbf{Code:} \texttt{cs336\_systems/profile\_memory.sh}

\begin{enumerate}[label=(\alph*)]
    
    \item \textbf{How do your memory timelines look like? Can you tell which stage is running based on the peaks you see?}
    
For the forward pass during inference, most memory has already been allocated and we see only small bursts.  The largest peaks corresponds to \texttt{softmax}.  
Smaller ones correspond to other transformer block operations, such as \texttt{SiLU}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/forward.png}
    \caption{Forward pass}
\end{figure}

For the full step, we see a small spike for \texttt{zero\_grad} in the beginning, then a gradual increase in memory use during the forward pass 
(store activations with spikes for softmax, etc.).
During the backward pass, we deallocate activations while allocating / reusing memory for gradients.  
At context length 128, the peak is during the backward pass (fewer attention activations), whereas at 512, the peak is achieved during the forward pass.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/training_step.png}
    \caption{Training step}
\end{figure}

    \item \textbf{What is the peak memory usage of each context length when doing a forward pass? What about when doing a full training step?}
    
    \textbf{Code:} \texttt{cs336\_systems/memory\_peaks.ipynb}

    There is not much difference between the context lengths 128 and 256 because in both cases memory is mostly used by the backward pass.

    \input{tables/peak_memory.tex}
    
    \item \textbf{Does mixed-precision significantly affect memory usage?}

Not significantly.  We instantiate the model in float32 and then use autocast for the forward pass, so we increase memory consumption during warmup - before taking measurements. 
At the same time, AMP decreases the size of activations, which matters only with larger contexts (we see the difference at 512).

    \item \textbf{At our reference hyperparameters, what is the size of a tensor of activations in the Transformer residual stream, in single-precision?}

    $\frac{d_k \times context\_length \times batch\_size \times bytes\_in\_float}{bytes\_in\_megabyte}=\frac{2560 \times 512 \times 4 \times 4}{1024^2}=20$ (MB).

    \item \textbf{What is the size of the largest allocations shown? Looking through the stack trace, can you tell where those allocations come from?}
    
    The largest alloctions have size 128 MB and come from attention's softmax.

    $\frac{batch\_size \times num\_heads \times context\_size^2 \times bytes\_in\_float}{bytes\_in\_megabyte} = \frac{4 \times 32 \times 512^2 \times 4}{1024^2} = 128$

\end{enumerate}

\subsection{Optimizing Attention with FlashAttention-2}
\subsubsection*{Problem (pytorch\_attention)}

\begin{enumerate}[label=(\alph*)]

    \item \textbf{Report the timings (or out-of-memory errors) you get for these configurations. At what size do
you get out-of-memory errors? Do the accounting for the memory usage of attention in one of the
smallest configurations you find that runs out of memory (you can use the equations for memory
usage of Transformers from Assignment 1). How does the memory saved for backward change
with the sequence length? What would you do to eliminate this memory cost?}

\textbf{Code:} 

\begin{itemize}
\item \texttt{cs336\_systems/benchmark\_attention.py}
\item \texttt{cs336\_systems/benchmark\_att.sh}
\item \texttt{cs336\_systems/attention.ipynb}
\end{itemize}

\input{tables/attention.tex}

On an H-100, we do not run out of memory from a single attention, though the memory usage increases dramatically
with the context length.  The memory saved is:

\begin{itemize}
\item inputs (Q, K, V): $context \times d_{\text{model}}$ each;
\item attention: $context^2$;
\item output: $context \times d_{\text{model}}$.
\end{itemize}

With batch size 8, context size 16384, and $d_{\text{model}}=128$, we have:

$\frac{4}{1024^2} \times (8 \times (16384^2 + 4 \times 16384 \times 128)) = 8448$ (MB)

This is close to the 8514 MB peak reported by pytorch.  While the inputs and the output are relatively small, with long contexts,
most memory is consumed by the intermediate attention matrix.  We need to perform the computation differently to avoid storing this 
huge intermediate result.

\end{enumerate}

\subsection{Benchmarking JIT-Compiled Attention}
\subsubsection*{Problem (torch\_compile)}

\begin{enumerate}[label=(\alph*)]

    \item \textbf{Extend your attention benchmarking script to include a compiled version of your PyTorch 
    implementation of attention, and compare its performance to the uncompiled version with the same
configuration as the \texttt{pytorch\_attention} problem above.}

See the tables above.  We do not save on memory, but there are significant savings on running time with larger contexts.

    \item \textbf{Now, compile your entire Transformer model in your end-to-end benchmarking script. How does
the performance of the forward pass change? What about the combined forward and backward
passes and optimizer steps?}

\textbf{Code:} 
\begin{itemize}
\item \texttt{cs336\_systems/benchmark\_comp.sh}
\item \texttt{cs336\_systems/compiled.ipynb}
\end{itemize}

\input{tables/compiled.tex}

The results are with batch size 4.  Compilation reduces the running time of both the forward and backward passes, 
but less so for larger models and contexts.  It has virtually no effect on the optimizer step because it's not compiled.

\end{enumerate}

\subsubsection*{Problem (flash\_forward)}
\textbf{Code:} 
\begin{itemize}
\item \texttt{cs336\_systems/flash\_pytorch.py}
\item \texttt{cs336\_systems/flash\_triton.py}
\end{itemize}

\subsubsection*{Problem (flash\_backward)}
\textbf{Code:} \texttt{cs336\_systems/flash\_triton.py}

\subsubsection*{Problem (flash\_benchmarking)}
\textbf{Code:}
\begin{itemize}
\item \texttt{cs336\_systems/benchmark\_flash.py}
\item \texttt{cs336\_systems/benchmark\_flash.sh}
\item \texttt{cs336\_systems/benchmark\_flash.ipynb}
\end{itemize}

The general conclusions are:

\begin{itemize}
\item The pytorch implementation is OK for short sequences, but takes increasingly more time with longer ones, eventually getting OOM in the backward
pass with sequence length 65536.
\item The forward pass of the two implementations of flash attention (compiled torch backward and triton backward passes) is the same, yet I got very
different results for the two.  I am not sure where this is coming from.
\item The Triton backward pass shines where parallelism matters: longer contexts and smaller inner dimensions.  The pytorch compiled version is better
in the opposite scenarios (shorter contexts / larger inner dimensions).
\end{itemize}

\input{tables/flash.tex}

\subsubsection*{Leaderboard}
On an H-100, I got 48.3ms with the Triton implementation of both forward and backward passes.  I did not perform any further improvements (except for autotuning),
so the result is expectedly worse than the top 5 leaderboard times.  With triton forward and pytorch backward passes, I got 83.7ms, which roughly corresponds
to the 80ms naïve baseline (with \texttt{torch.set\_float32\_matmul\_precision("high")}).
\setcounter{section}{2}
\setcounter{subsection}{0}

\subsection{Single-Node Distributed Communication in PyTorch}
\subsubsection*{Problem (distributed\_communication\_single\_node)}

\textbf{Code:} 
\begin{itemize}
\item \texttt{cs336\_systems/single\_node.sh}
\item \texttt{cs336\_systems/single\_node.py}
\item \texttt{cs336\_systems/single\_node.ipynb}
\end{itemize}

Using an 8-GPU 40GB A100 SXM4 instance, I got the following results:

\input{tables/single_node.tex}

Comments:
\begin{itemize}
\item Using GPUs (NCCL) is orders of magnitude faster than using a CPU (NCCL).
\item Time grows significantly (roughly linearly) with tensor size - more rapidly on a CPU.
\item Time grows mildly with the world size - again more rapidly on a CPU.
\end{itemize}

\subsection{A Naïve Implementation of Distributed Data Parallel Training}

\textbf{Code:} 
\begin{itemize}
\item \texttt{cs336\_systems/ddp\_benchmark.py}
\item \texttt{cs336\_systems/ddp\_benchmark.sh}
\end{itemize}

\subsubsection*{Problem (naive\_ddp\_benchmarking)}

TODO

\subsection{Improving Upon the Minimal DDP Implementation}

\textbf{Code:} 
\begin{itemize}
\item \texttt{cs336\_systems/ddp\_benchmark.py}
\item \texttt{cs336\_systems/ddp\_benchmark.sh}
\end{itemize}

\subsubsection*{Problem (minimal\_ddp\_flat\_benchmarking)}

TODO

\subsubsection*{Problem (ddp\_overlap\_individual\_parameters)}

TODO

\subsubsection*{Problem (ddp\_overlap\_individual\_parameters\_benchmarking)}

TODO

\subsubsection*{Problem (ddp\_overlap\_bucketed)}

TODO

\subsubsection*{Problem (ddp\_bucketed\_benchmarking)}
\begin{enumerate}[label=(\alph*)]
    \item \textbf{Benchmark your bucketed DDP implementation using the same config as the previous experiments
(1 node, 2 GPUs, XL model size), varying the maximum bucket size (1, 10, 100, 1000 MB).
Compare your results to the previous experiments without bucketing—do the results align with
your expectations? If they don't align, why not? You may have to use the PyTorch profiler as
necessary to better understand how communication calls are ordered and/or executed. What
changes in the experimental setup would you expect to yield results that are aligned with your
expectations?}

TODO

    \item \textbf{Assume that the time it takes to compute the gradients for a bucket is identical to the time it
takes to communicate the gradient buckets. Write an equation that models the communication
overhead of DDP (i.e., the amount of additional time spent after the backward pass) as a function
of the total size (bytes) of the model parameters ($s$), the all-reduce algorithm bandwidth ($w$,
computed as the size of each rank's data divided by the time it takes to finish the all-reduce), the
overhead (seconds) associated with each communication call ($o$), and the number of buckets ($n_b$).
From this equation, write an equation for the optimal bucket size that minimizes DDP overhead.}

For simplicity, let us make an additional assumption that all the buckets have the same size $\frac{s}{n_b}$.
Then the DDP overhead is composed of the overhead per bucket plus the time it takes to communicate the last bucket:

$\boxed{T = n_bo + \frac{s}{n_bw}}$

$T \rightarrow \infty$ when $n_b \rightarrow 0$ or $n_b \rightarrow \infty$; therefore the minimimum is achieved when $\frac{dT}{dn_b}=0$:

$\frac{dT}{dn_b}=0 \Leftrightarrow o - \frac{s}{n_b^2w} = 0 \Leftrightarrow n_b = \sqrt{\frac{s}{ow}}$

Therefore, the optimal batch size is $\frac{s}{\sqrt{\frac{s}{ow}}}$: $\boxed{B = \sqrt{sow}}$.

\end{enumerate}

\subsection{4D Parallelism}
\subsubsection*{Problem (communication\_accounting)}
\begin{enumerate}[label=(\alph*)]
    \item \textbf{How much memory would it take to store the master model weights, accumulated gradients and
optimizer states in FP32 on a single device? How much memory is saved for backward (these will
be in BF16)? How many H100 80GB GPUs worth of memory is this?}

Number of weights: $W = 2 \times num\_blocks \times d_{\text{model}} \times d_{\text{ff}} = 3276 \times 2^{26}$

Memory for weights, accumulated gradients, and optimizer states: 

$M_1 = \frac{4 \times 4 \times W}{2^{30}} = 3276$ (GB)

Activations per token: 

$A = num\_blocks \times (d_{\text{model}} + d_{\text{ff}}) = 1071 \times 2^{13}$

For the given batch size ($B$) and sequence length ($L$), the memory needed for activations is:

$M_2 = \frac{2 \times BL \times A}{2^{30}} = \frac{1071BL}{2^{16}}$ (GB)

The number of H100 80GB GPUs needed:

$N = \left\lceil\frac{3276 + \frac{1071BL}{2^{16}}}{80}\right\rceil$

For our huge model, if we have 1M tokens/step, e.g. $B=512$ and $L=2048$, then 

$N=\left\lceil\frac{3276 + 17136}{80}\right\rceil = 256$ (H100 80GB GPUs)

\item \textbf{Now assume your master weights, optimizer state, gradients and half of your activations (in
practice every second layer) are sharded across $N_{\text{FSDP}}$ devices. Write an expression for how
much memory this would take per device. What value does $N_{\text{FSDP}}$ need to be for the total
memory cost to be less than 1 v5p TPU (95GB per device)?}

Per device, we would need 

$M_{\text{device}} = \frac{M_1 + \frac{M_2}{2}}{N_{\text{FSDP}}} = \frac{3276 + \frac{1071BL}{2^{17}}}{N_{\text{FSDP}}}$ (GB)

For $M_{\text{device}} \leq 95$, we would need at least $N_{\text{FSDP}} = \left\lceil \frac{3276 + \frac{1071BL}{2^{17}}}{95} \right\rceil$ devices.

For example, with our earlier choice of $BL=2^{20}$, we get 

$N_{\text{FSDP}} = \left\lceil \frac{3276 + 1071 \times 8}{95} \right\rceil = 125$ (devices).

\item \textbf{Consider only the forward pass. Use the communication bandwidth of $W_{\text{ici}} = 2 \cdot 9 \cdot 10^{10}$ and
FLOPS/s of $C = 4.6 \cdot 10^{14}$ for TPU v5p as given in the TPU Scaling Book. Following the
notation of the Scaling Book, use $M_X = 2$, $M_Y = 1$ (a 3D mesh), with $X = 16$ being your FSDP
dimension, and $Y = 4$ being your TP dimension. At what per-device batch size is this model
compute bound? What is the overall batch size in this setting?}

The TPU scaling book gives the following compute-bound inequality:

$\frac{B}{N} > \frac{\alpha^2}{M_XM_YF}$

Here, $N=XY$ is the total number of chips, $\alpha=\frac{C}{W_{\text{ici}}}$ is the ICI arithmetic intensity, and $F=d_{\text{ff}}$ is the feed-forward dimension.
Thus, $\frac{B}{N} > 61.325$.  For the microbatch size of 62, the overall batch size is $62 \times 16 \times 4 = 3968$.

\item \textbf{In practice, we want the overall batch size to be as small as possible, and we also always use our
compute effectively (in other words we want to never be communication bound).  
What other tricks can we employ to reduce the batch size of our model but retain high throughput?}

The batch should be large enough to have low noise per step; however, once the noise is low, further increasing the batch size will result in 
wasteful computation.

$eff\_batch = microbatch \times grad\_acc \times world\_size$.

To keep the effective batch small while retaining high throughput:

\begin{itemize}
\item Increase the microbatch size and reduce the frequency of gradient 
synchronization, and / or choose a smaller data-parallel world size.

\item Use activation checkpointing, i.e. recompute activations instead of storing them to help us increase the microbatch size without running out of memory.

\item Use other types of model parallelism, such as pipeline and expert (if relevant), to help us remain computation bound without increasing the batch size.
\end{itemize}

\end{enumerate}
{
\renewcommand{\thesubsection}{3}
\subsection{Optimizer State Sharding}
}

\subsubsection*{Problem (optimizer\_state\_sharding)}

\textbf{Code:} \texttt{cs336\_systems/sharded\_optimizer.py}

\subsubsection*{Problem (optimizer\_state\_sharding\_accounting)}

\begin{enumerate}[label=(\alph*)]
\item \textbf{Create a script to profile the peak memory usage when training language models with and without
optimizer state sharding. Using the standard configuration (1 node, 2 GPUs, XL model size),
34
report the peak memory usage after model initialization, directly before the optimizer step, and
directly after the optimizer step. Do the results align with your expectations? Break down the
memory usage in each setting (e.g., how much memory for parameters, how much for optimizer
states, etc.).}

TODO

\item \textbf{How does our implementation of optimizer state sharding affect training speed?}

TODO

\item \textbf{How does our approach to optimizer state sharding differ from ZeRO stage 1 (described as ZeRO-
DP $P_{os}$ in Rajbhandari et al., 2020)?}

Conceptually, it is the same as ZeRO stage 1; however, the implementation is simplified.  They write:

\begin{quotation}
We perform an all-gather across the data parallel
process at the end of each training step to get the fully updated parameters across all data
parallel process.
\end{quotation}

Our approach broadcasts every parameter individually, which increases the communication cost.
Doing an all-gather would require all input tensors (one per rank) to have the same shape and the same datatype.
This could be achieved by flattening / unflattenting parameters using per-datatype buckets.

\end{enumerate}

\end{document}